<!DOCTYPE html>
<!--
    Plain-Academic by Vasilios Mavroudis
    Released under the  Simplified BSD License/FreeBSD (2-clause) License.
    https://github.com/mavroudisv/plain-academic
-->

<html lang="en">
<head>
  <title>Quynh nguyen</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.0/jquery.min.js"></script>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css">
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
  <link href='https://fonts.googleapis.com/css?family=Oswald:700' rel='stylesheet' type='text/css'>
</head>
<body>


<!-- Navigation -->
    <nav class="navbar navbar-inverse style="margin-left: 100px;"">
      <div class="container">
		<ul class="nav navbar-nav">
              <li><a href="#">Home</a></li>
              <li><a href="#publications">Publications</a></li>
              <li><a href="#teaching">Teaching</a></li>
              <li><a href="#talks">Talks</a></li>
		</ul>
	  </div>
	</nav>
  <!-- Page Content -->
    <div class="container">
        <div class="row">
            <!-- Contact Info on the Sidebar -->
            <div class="col-md-3 col-xs-12">
                <div style="word-wrap: break-word; font-family: 'Oswald', sans-serif; font-size: 32px;"><b>Quynh Nguyen</b></div><br>
                <div> <p><br> </div>
                <div class="row"> <div class="col-md-10 col-xs-4"> <img class="img-responsive" src="face.jpeg" alt=""></div></div><br>
                <div> <p>Email: quynhnguyenngoc89@gmail.com<br> </div>
            </div>
            <!-- Entries Column -->
            <div class="col-md-8 col-xs-12">
                <div class="col" style="text-align:justify">

                    <h3>Provable guarantees for training neural networks</h3>

                    <ul>
                    <li>
                    <b><a href="https://arxiv.org/abs/2101.09612" target="_blank">On the Proof of Global Convergence of Gradient Descent for Deep ReLU Networks with Linear Widths</a></b><br>
                    <b>Quynh Nguyen</b>. ICML 2021
                    <p>This article provides a short proof for the global convergence of GD in training deep ReLU networks.
                    For arbitrary labels, it is shown that a linear, quadratic or cubic width suffices to prove the result (depending on the initilization).</p>
                    </li>

                    <li>
                      <b><a href="https://arxiv.org/abs/2002.07867" target="_blank">Global Convergence of Deep Networks with One Wide Layer Followed by Pyramidal Topology</a></b><br>
                    <b>Quynh Nguyen</b> and Marco Mondelli. ICML 2020
                    <p>Earlier works showed that gradient descent (GD) can find a global solution
                      when all the hidden layers are polynomially wide.
                    However this condition makes neural networks operate in a kernel regime.
                    This article shows that global convergence can be proved for deep pyramidal nets -- a much more empirically relevant architecture
                    where only the first hidden layer needs to be wide and the remaining hidden layers can have constant and non-increasing widths.
                    In this pyramidal setting, GD provably moves the feature representations of the network by at least &#937;(1),
                    and hence training goes beyond the NTK/lazy regime where this change is typically o(1).
                    </p>
                    </li>

                    <li>
                    <b><a href="https://arxiv.org/abs/1610.09300" target="_blank">Globally Optimal Training of Generalized Polynomial Neural Networks with Nonlinear Spectral Methods</a></b><br>
                    Antoine Gautier, <b>Quynh Nguyen</b> and Matthias Hein. NIPS 2016
                    <p>We cast the optimization problem of a polynomial network as a nonlinear eigenvalue problem.
                    We study the uniqueness and global optimality of the solution, and propose a generalized power method to solve it.</p>
                    </li>
                    </ul>



                    <h3>Loss surface, optimization landscape, sublevel sets</h3>

                    <ul>
                    <li>
                    <b><a href="https://arxiv.org/abs/2101.08576" target="_blank">A Note on Connectivity of Sublevel Sets in Deep Learning</a></b><br>
                    <b>Quynh Nguyen</b>. Technical note, 2021
                    <p>For shallow networks, it is shown that having N+1 hidden neurons is both necessary and sufficient for the training loss function of neural networks to have connected sublevel sets.
                    For deeper architecture, this condition is shown to be sufficient.
                    However, whether it is necessary or not for multilayer networks is still an open problem.
                    </p>
                    </li>


                    <li>
                    <b><a href="https://arxiv.org/abs/2102.09671" target="_blank">When Are Solutions Connected in Deep Networks?</a></b><br>
                    <b>Quynh Nguyen</b>, Pierre Brechet and Marco Mondelli. NeurIPS 2021
                    <p>This article gives a condition for which certain points in parameter space
                    can be connected by a continuous path along which there are no barriers or jumps in the loss landscape.
                    This sounds similar to results on connected sublevel sets, but it is weaker in the sense that the connectivity is only shown for a subset of solutions.
                    At the same time, the requirement on over-parameterization is also weaker.
                    Empirically, it is found that the provided condition can capture well the mode connectivity phenomenon concerning SGD solutions in deep learning.
                   </p>
                    </li>

                    <li><b><a href="https://arxiv.org/abs/1901.07417" target="_blank">On Connected Sublevel Sets in Deep Learning</a></b><br>
                    <b>Quynh Nguyen</b>. ICML 2019
                    <p>This article proves the connectivity of sublevel sets of the loss function of deep pyramidal networks.</p>
                    </li>

                    <li>
                    <b><a href="https://arxiv.org/abs/1809.10749" target="_blank">On the Loss Landscape of a Class of Deep Neural Networks with No Bad Local Valleys</a></b><br>
                    <b>Quynh Nguyen</b>, Mahesh Chandra Mukkamala and Matthias Hein. ICLR 2019
                    <p>Consider neural networks as a directed acyclic graph, this article shows that the loss function has no spurious valleys as long as there are enough skip-connections from lower layers to the output layer.
                      Empirically, it is shown that adding random skip-connections from lower layers to the output
                    can remove not only spurious valleys but also vanishing gradient issues,
                     which makes the training of very deep networks much more stable and efficient.
                    </p>
                    </li>

                    <li>
                    <b><a href="https://arxiv.org/abs/1803.00094" target="_blank">Neural Networks Should Be Wide Enough to Learn Disconnected Decision Regions</a></b><br>
                    <b>Quynh Nguyen</b>, Mahesh Mukkamala and Matthias Hein. ICML 2018
                    <p>In order for neural networks to learn disconnected decision regions in the input space, at least one of the hidden layers should have more neurons than the input dimension.</p>
                    </li>

                    <li>
                    <b><a href="https://arxiv.org/abs/1710.10928" target="_blank">Optimization Landscape and Expressivity of Deep CNNs</a></b><br>
                    <b>Quynh Nguyen</b> and Matthias Hein. ICML 2018
                    <p>
                      This article shows that a standard convolutional layer suffices to memorize any N samples as long as the number of parameters exceeds N.
                      It also provides a condition for global optimality of critical points in deep CNNs.</p>
                    </li>

                    <li>
                    <b><a href="https://arxiv.org/abs/1704.08045" target="_blank">The Loss Surface of Deep and Wide Neural Networks</a></b><br>
                    <b>Quynh Nguyen</b> and Matthias Hein. ICML 2017
                    <p>This article studies the global optimality of local minima for deep nonlinear networks.
                    The proof exploits Implicit Function Theorem to characterize the optimality of local minima in terms of their non-degenerate conditions.
                  </p>
                    </li>

                    </ul>



                    <h3>Neural tangent kernel</h3>

                    <ul>
                    <li>
                    <b><a href="https://arxiv.org/abs/2012.11654" target="_blank">Tight Bounds on the Smallest Eigenvalue of the Neural Tangent Kernel for Deep ReLU Networks</a></b><br>
                    <b>Quynh Nguyen</b>, Marco Mondelli and Guido Montufar. ICML 2021
                    </li>
                    <p>
                      The spectrum of the NTK has found applications in proving memorization capacity, convergence of GD and generalization bounds in certain regimes.
                      This article provides tight lower bounds on the smallest eigenvalue of the NTK matrix for Gaussian weights,
                      both in the limit of infinitely wide networks, and for finite-width networks.
                    </p>
                    </ul>


                    <h3>Initialization of deep networks</h3>
                    <ul>
                    <li>
                    <b><a href="https://arxiv.org/abs/2101.12017" target="_blank">A Fully Rigorous Proof of the Derivation of Xavier and He's Initialization for Deep ReLU Networks</a></b><br>
                    <b>Quynh Nguyen</b>. Technical note, 2021
                    </li>
                    <p>He's and LeCun's initialization are very popular methods for initializing neural network weights in deep learning.
                    However, the formulas of these initializations in the original papers have been only derived under the assumption that all the hidden neurons are somewhat independent
                    -- a condition known to be satisfied only for infinitely wide networks.
                    This article provides a rigorous derivation for the case of networks with flinite abeit large widths.</p>
                    </ul>


                    <h3>Nonconvex optimization</h3>
                    <ul>
                    <li>
                    <b><a href="https://www.ml.uni-saarland.de/Publications/NgGauHei-OPT2016.pdf" target="_blank">Nonlinear Spectral Methods for Nonconvex Optimization with Global Optimality</a></b><br>
                    <b>Quynh Nguyen</b>, Antoine Gautier and Matthias hein. NIPS Workshop on Optimization, 2016
                    <p>This extends our NIPS'16 paper on polynomial networks to more general optimization problems.</p>
                    </li>
                    </ul>


                    <h3>Computer vision</h3>
                    <ul>
                    <li>
                    <b><a href="https://arxiv.org/abs/1511.02667" target="_blank">An Efficient Multilinear Optimization Framework for Hypergraph Matching</a></b><br>
                    <b>Quynh nguyen</b>, Francesco Tudisco, Antoine Gautier and Matthias Hein. T-PAMI 2017
                    <p>This is an extension of our CVPR paper on hypergraph matching.</p>
                    </li>

                    <li>
                    <b><a href="https://arxiv.org/abs/1504.07907" target="_blank">A Flexible Tensor Block Coordinate Ascent Scheme for Hypergraph Matching</a></b><br>
                    <b>Quynh Nguyen</b>, Antoine Gautier and Matthias Hein. CVPR 2015
                    <p>This article studies hypergraph matching through the lens of multilinear optimization.</p>
                    </li>

                    <li>
                    <b><a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Xian_Latent_Embeddings_for_CVPR_2016_paper.pdf" target="_blank">Latent Embeddings for Zero-shot Classification</a></b><br>
                    Yongqin Xian, Zeynep Akata, Gaurav Sharma, <b>Quynh Nguyen</b>, Matthias Hein and Bernt Schiele. CVPR 2016
                    </li>
                    <p>This article proposes a model for learning the compatibility of data and class embeddings for zero shot leanring.</p>
                    </ul>


                </div>

                <div class="col">
                    <h2 id="teaching">Teaching</h2>
                        <p><b>Convex Optimization</b> </p>
                        <p><b>Advanced Topics in Machine Learning</b> (Seminar)</p>
                </div>

                <div class="col">
                    <h2 id="talks">Talks</h2>
                        <p><b>Loss surface of deep and wide neural networks</b><br>Math Machine Learning seminar at MPI-MIS and UCLA, (virtual) 2020</p>
                        <p><b>Optimization landscape of deep neural networks</b><br>Simons Institute for the Theory of Computing, Berkeley, California. 2019</p>
                        <p><b>Optimization landscape of deep CNNs</b><br>Microsoft Research Redmond (MSR) 2018</p>
                </div>
            </div>
    </div>
    </div>

</body>

</html>
